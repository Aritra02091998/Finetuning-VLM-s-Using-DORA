{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b36b706f-34ba-4338-b841-6afcf60aa426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stable_env\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(os.getenv(\"CONDA_DEFAULT_ENV\"))\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630c9b73-493d-422d-8ef5-b964afaa84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import datasets\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "207e3336-6f77-4c7d-8cc3-8a0ba7cad759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset\n",
    "from functools import partial\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "raw",
   "id": "405aa65e-601d-45a1-99d0-07542e041da6",
   "metadata": {},
   "source": [
    "## For utilizing Image and Video Processing Script of Qwen-VL\n",
    "\n",
    "!pip install qwen-vl-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e408bf84-2f39-4437-a2bb-a34ab994cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.vision_util import process_vision_info\n",
    "from util.logutil import init_logger, get_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c758b-a507-42f4-8037-490988abb846",
   "metadata": {},
   "source": [
    "### Load the MultiDomain Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cd9f09b-fc65-417f-8e73-3724fea9a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Generate a one word or single number answer for the given image and question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51b94ba9-c46e-473e-9215-a78242016e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_prefix(example):\n",
    "    example['question'] = prefix + ': ' + example['question']\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a431a0d-a0d5-4df8-b0c1-5c52378fe004",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dutta18/multi-domain-VQA-1.5K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd66b39-2b6c-4bbf-867f-3a5fedfd029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = dataset['train'], dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64719b62-44e7-4831-bfe4-058a9518ef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.map(prepend_prefix)\n",
    "val_set = val_set.map(prepend_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c46ef8e-e056-41f0-989b-d7fcd0ea396f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 600)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcaf610-a654-4d4f-88ff-7decf13942ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eda11bc6-dc23-4d0c-afaf-198deb92a31a",
   "metadata": {},
   "source": [
    "### Creating JSON Format of the AOKVQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9038e3d-a1a3-4454-b212-99ca3cbd1e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064e5b6e999241c9b72da5f15e391b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formattedJSONTrain = list()\n",
    "\n",
    "for idx in tqdm(range(len(train_set))):\n",
    "    currentJSON =   {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"image\": train_set[idx]['image']\n",
    "                        },\n",
    "                        {\"type\": \"text\", \"text\": f\"{train_set[idx]['question']}\"}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": f\"{train_set[idx]['answer']}\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }  \n",
    "    formattedJSONTrain.append(currentJSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b070efa6-e6bf-4c47-99b7-0c3928f0f50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56714372ab4f4f258992b122040b658e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formattedJSONVal = list()\n",
    "\n",
    "for idx in tqdm(range(len(val_set))):\n",
    "    currentJSON =   {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"image\": val_set[idx]['image']\n",
    "                        },\n",
    "                        {\"type\": \"text\", \"text\": f\"{val_set[idx]['question']}\"}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": f\"{val_set[idx]['answer']}\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }  \n",
    "    formattedJSONVal.append(currentJSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0b2115c-44f7-4cd6-93a4-1bd0168496e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'messages': [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=792x525>},\n",
       "     {'type': 'text',\n",
       "      'text': 'Generate a one word or single number answer for the given image and question: how many antitrypsin was talc used to sclerose emphysematous lung, alpha-deficiency?'}]},\n",
       "   {'role': 'assistant', 'content': [{'type': 'text', 'text': '1'}]}]},\n",
       " {'messages': [{'role': 'user',\n",
       "    'content': [{'type': 'image',\n",
       "      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x427>},\n",
       "     {'type': 'text',\n",
       "      'text': 'Generate a one word or single number answer for the given image and question: How many bowl are there?'}]},\n",
       "   {'role': 'assistant', 'content': [{'type': 'text', 'text': 'one'}]}]})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formattedJSONTrain[0], formattedJSONVal[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e8b9bdf-d33d-4a1e-81ae-770fc3c64c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'train_output/{datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")}/'\n",
    "init_logger(output_dir)\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6463ce05-1898-4d8c-bbbd-c6dd4622f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a994a-e9e0-47ec-9923-de33f12d93b2",
   "metadata": {},
   "source": [
    "### Prepare Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88da1581-779d-4c1f-bd35-b55376e16f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bdb84cd-56ed-4cdb-9c2b-c83d4287b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class aokvqa(Dataset):\n",
    "    def __init__(self, formatted_json_data):\n",
    "        super().__init__()\n",
    "        self.data = formatted_json_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c346273f-9cd0-4aa3-aa5d-4b6932e8b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = aokvqa(formattedJSONTrain)\n",
    "val_dataset = aokvqa(formattedJSONVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854fb87-5a18-4a8a-9d71-859511895138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a22b92f-5cee-4c71-930e-0a6c973b1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_assistant_content_sublist_indexes(l):\n",
    "    '''\n",
    "    A message from train_data/data.json may look like below:\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {'role': 'user', 'content': [{'type': 'image', 'image': 'train_data/1.jpeg'}, {'type': 'text', 'text': '描述一下这个图片'}]}, \n",
    "                {'role': 'assistant', 'content': [{'type': 'text', 'text': '这张图片展示了一位年轻女子和她的狗在海滩上玩耍的场景。女子穿着格子衬衫和黑色裤子，坐在沙滩上，与她的金毛犬互动。她们的手臂伸展着，似乎在进行某种游戏或训练。背景是广阔的海洋和晴朗的天空，阳光洒在沙滩上，营造出温暖而宁静的氛围。整体画面充满了快乐和放松的感觉。'}]}\n",
    "            ]\n",
    "        }\n",
    "    After apply_chat_template, the text will look like below:\n",
    "        ['<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>描述一下这个图片<|im_end|>\\n<|im_start|>assistant\\n这张图片展示了一位年轻女子和她的狗在海滩上玩耍的场景。女子穿着格子衬衫和黑色裤子，坐在沙滩上，与她的金毛犬互动。她们的手臂伸展着，似乎在进行某种游戏或训练。背景是广阔的海洋和晴朗的天空，阳光洒在沙滩上，营造出温暖而宁静的氛围。整体画面充满了快乐和放松的感觉。<|im_end|>\\n']\n",
    "\n",
    "    This function tries to find the indexes of the assistant content in the input_ids list to build labels.\n",
    "    '''\n",
    "    start_indexes = []\n",
    "    end_indexes = []\n",
    "\n",
    "    # Iterate through the list to find starting points\n",
    "    for i in range(len(l) - 1):\n",
    "        # Check if the current and next elements form the start sequence\n",
    "        if l[i] == 151644 and l[i+1] == 77091 and l[i+2] == 198:\n",
    "            start_indexes.append(i+3)\n",
    "            # Now look for the first 151645 and 198 after the start\n",
    "            for j in range(i+3, len(l)-1):\n",
    "                if l[j] == 151645 and l[j+1] == 198:\n",
    "                    end_indexes.append(j+2) # **NOTE** the <|im_end|>\\n 2 tokens should be included in the label, so that model can predicate end of output.\n",
    "                    break  # Move to the next start after finding the end\n",
    "\n",
    "    return list(zip(start_indexes, end_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4274e9b9-9228-48f2-81cd-b6ead522b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, processor, device):\n",
    "    \n",
    "    messages = [m['messages'] for m in batch]\n",
    "    texts = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=False) for msg in messages]\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    input_ids_lists = inputs['input_ids'].tolist()\n",
    "    assert len(messages) == len(input_ids_lists)\n",
    "\n",
    "    labels_list = []\n",
    "    for ids_list in input_ids_lists:\n",
    "        label_ids = [-100] * len(ids_list)\n",
    "        for begin_end_indexs in find_assistant_content_sublist_indexes(ids_list):\n",
    "            label_ids[begin_end_indexs[0]:begin_end_indexs[1]] = ids_list[begin_end_indexs[0]:begin_end_indexs[1]]\n",
    "        labels_list.append(label_ids)\n",
    "\n",
    "    labels_ids = torch.tensor(labels_list, dtype=torch.int64)\n",
    "    return inputs, labels_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92b3cd4d-c5a6-477c-9f76-05efb4a98bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_chat_template(processor, output_dir):\n",
    "    '''\n",
    "    ***Note**\n",
    "\n",
    "    We should have not had this function, as normal processor.save_pretrained(output_dir) would save chat_template.json file.\n",
    "    However, on 2024/09/05, I think a commit introduced a bug to \"huggingface/transformers\", which caused the chat_template.json file not to be saved. \n",
    "    See the below commit, src/transformers/processing_utils.py line 393, this commit avoided chat_template.json to be saved.\n",
    "    https://github.com/huggingface/transformers/commit/43df47d8e78238021a4273746fc469336f948314#diff-6505546ec5a9ab74b2ce6511681dd31194eb91e9fa3ce26282e487a5e61f9356\n",
    "\n",
    "    To walk around that bug, we need manually save the chat_template.json file.\n",
    "\n",
    "    I hope this bug will be fixed soon and I can remove this function then.\n",
    "    '''\n",
    "    output_chat_template_file = os.path.join(output_dir, \"chat_template.json\")\n",
    "    chat_template_json_string = json.dumps({\"chat_template\": processor.chat_template}, indent=2, sort_keys=True) + \"\\n\"\n",
    "    with open(output_chat_template_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "        writer.write(chat_template_json_string)\n",
    "        logger.info(f\"chat template saved in {output_chat_template_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80285200-b384-4fe5-ac83-3411e841691f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d76bce9-58e3-4045-9096-247b713eedcd",
   "metadata": {},
   "source": [
    "### Initialize Quantization Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0aaafc1f-065f-4578-8596-6a34ff1aae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e59b6e1c-d112-41d4-a896-7bbd9d99f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enable 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Use float16 for computation\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization for memory savings\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Use NormalFloat4 (NF4) quantization type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96192a2-fecd-4277-b8e7-9d39e483db4e",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34c03787-c6eb-44fd-8b05-d847788e81f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe832db59c594db691828f21db490d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage = True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "# Load processor. \n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=256*28*28, max_pixels=512*28*28, padding_side=\"left\", use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2448d86-77d3-4f57-a788-d65fabbe1986",
   "metadata": {},
   "source": [
    "### Initialize DORA Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57898f7b-ae0c-4d29-b9a5-5d1dd6cdfc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8*2,  # Scaling factor\n",
    "    lora_dropout=0.05,  # Dropout rate\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"qkv\", \"attn.proj\"], \n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    use_dora = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e9a83-59f6-494c-a016-264da818bfdb",
   "metadata": {},
   "source": [
    "#### Apply Grad Checkpointing using and optimizations: prepare_model_for_kbit_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50543354-e20c-4b33-b833-477917d0daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "qdora_qwen_model = get_peft_model(model, dora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd9e0fb5-96e2-4009-9b72-914feafb7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#qdora_qwen_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fa3f60-4604-44cc-8dbd-2045eb1ed3e2",
   "metadata": {},
   "source": [
    "### Report Parameter Size: ~ 12.0 M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51288b20-7a5e-41de-a91f-da07aa4221fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_trainable_params():\n",
    "    \n",
    "    trainable = sum(p.numel() for p in qdora_qwen_model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable params: {trainable/1e6:.1f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "527d3f2a-10c7-4f4f-8eb0-1f1700704b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable params: 12.0 M\n"
     ]
    }
   ],
   "source": [
    "report_trainable_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23eae2-9e85-45ec-9297-2305ae39b132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e3d87d8-29b8-43c3-8c52-aa17d6dccc75",
   "metadata": {},
   "source": [
    "### Create & Test Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "467f05ab-04fb-49d3-94f8-f7a6e9cfb797",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize_ = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1408b95-4f05-4568-8db1-fd50990098fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batchSize_,\n",
    "    collate_fn=partial(collate_fn, processor=processor, device=device),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batchSize_,\n",
    "    collate_fn=partial(collate_fn, processor=processor, device=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08af97d-d62d-4f4f-980b-2f39df08430f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46ea2c16-69a1-4ff0-ba2d-90743717aa1f",
   "metadata": {},
   "source": [
    "### Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c328c41e-562f-4e61-b3da-ef0810626c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_validation():\n",
    "    \n",
    "    qdora_qwen_model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            inputs, labels = batch\n",
    "            outputs = qdora_qwen_model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    qdora_qwen_model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2c2f5-63f5-4c91-b373-8f1f6e8e1c91",
   "metadata": {},
   "source": [
    "### Training Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c081744d-5fed-4acc-b578-bee8319e7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76d7c82d-a4b8-4a08-87d0-087e4db099ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "epochs = 5\n",
    "weight_decay = 0.01\n",
    "gradient_accumulation_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7926b984-8314-4547-bb03-bd80b04af485",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "best_val_loss = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5ec62d9-9664-48a7-9205-4a7124084c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch     = len(train_loader) // gradient_accumulation_steps\n",
    "total_train_steps   = steps_per_epoch * epochs\n",
    "num_warmup_steps    = int(0.05 * total_train_steps)          # 5 %   (quick)  \n",
    "# ➟ for medium: 0.03 works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71a30d26-b242-44c3-87fe-2fdd4cb4e517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "935 46\n"
     ]
    }
   ],
   "source": [
    "print(total_train_steps, num_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf509f09-5fee-48fd-bdd3-f1f2739b5e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(qdora_qwen_model.parameters(), lr=LR, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "293fd2d7-25ca-4b63-9ee9-bf1a77fde21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=total_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db64018e-ab1b-42ae-b02e-234df2bbbbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDir = '/home/aritrad/main/Qwen2-VL-2B/MOE/Multidomain/chkpts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4edaf507-ab8a-4337-8195-df1db5dcf9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdora_qwen_model.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6737c0f0-7809-4cd2-a0b0-6379bfcdcfa0",
   "metadata": {},
   "source": [
    "## Native PyTorch Training Loop\n",
    "\n",
    "##### I am using val_loss as the checkpointing criteria, but any other metric which test text generation quality can be used here.\n",
    "##### MAX GPU USAGE = 22 GB on NVIDIA A40 Card (Adjust LORA/DORA Rank, batch size and accumulation steps accordingly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea363c42-aeb5-4b25-9a0e-bee5677cf310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85dc598843b43fea38e652141057365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n",
      "2025-07-08 05:43:31-3080550328.py:20-INFO >> [ Epoch 1 | idx: 1 | Optim Step 1 | Train Loss: 0.6400 ]\n",
      "2025-07-08 05:43:38-3080550328.py:20-INFO >> [ Epoch 1 | idx: 3 | Optim Step 2 | Train Loss: 0.4567 ]\n",
      "2025-07-08 05:43:46-3080550328.py:20-INFO >> [ Epoch 1 | idx: 5 | Optim Step 3 | Train Loss: 1.1952 ]\n",
      "2025-07-08 05:43:53-3080550328.py:20-INFO >> [ Epoch 1 | idx: 7 | Optim Step 4 | Train Loss: 1.3454 ]\n",
      "2025-07-08 05:44:01-3080550328.py:20-INFO >> [ Epoch 1 | idx: 9 | Optim Step 5 | Train Loss: 0.7239 ]\n",
      "2025-07-08 05:44:08-3080550328.py:20-INFO >> [ Epoch 1 | idx: 11 | Optim Step 6 | Train Loss: 1.2187 ]\n",
      "2025-07-08 05:44:16-3080550328.py:20-INFO >> [ Epoch 1 | idx: 13 | Optim Step 7 | Train Loss: 2.7951 ]\n",
      "2025-07-08 05:44:23-3080550328.py:20-INFO >> [ Epoch 1 | idx: 15 | Optim Step 8 | Train Loss: 0.7461 ]\n",
      "2025-07-08 05:44:30-3080550328.py:20-INFO >> [ Epoch 1 | idx: 17 | Optim Step 9 | Train Loss: 0.8016 ]\n",
      "2025-07-08 05:44:39-3080550328.py:20-INFO >> [ Epoch 1 | idx: 19 | Optim Step 10 | Train Loss: 1.7977 ]\n",
      "2025-07-08 05:44:46-3080550328.py:20-INFO >> [ Epoch 1 | idx: 21 | Optim Step 11 | Train Loss: 1.2773 ]\n",
      "2025-07-08 05:44:54-3080550328.py:20-INFO >> [ Epoch 1 | idx: 23 | Optim Step 12 | Train Loss: 0.8487 ]\n",
      "2025-07-08 05:45:02-3080550328.py:20-INFO >> [ Epoch 1 | idx: 25 | Optim Step 13 | Train Loss: 1.7664 ]\n",
      "2025-07-08 05:45:09-3080550328.py:20-INFO >> [ Epoch 1 | idx: 27 | Optim Step 14 | Train Loss: 1.0880 ]\n",
      "2025-07-08 05:45:16-3080550328.py:20-INFO >> [ Epoch 1 | idx: 29 | Optim Step 15 | Train Loss: 1.2408 ]\n",
      "2025-07-08 05:45:23-3080550328.py:20-INFO >> [ Epoch 1 | idx: 31 | Optim Step 16 | Train Loss: 0.8855 ]\n",
      "2025-07-08 05:45:29-3080550328.py:20-INFO >> [ Epoch 1 | idx: 33 | Optim Step 17 | Train Loss: 0.6608 ]\n",
      "2025-07-08 05:45:36-3080550328.py:20-INFO >> [ Epoch 1 | idx: 35 | Optim Step 18 | Train Loss: 1.0005 ]\n",
      "2025-07-08 05:45:43-3080550328.py:20-INFO >> [ Epoch 1 | idx: 37 | Optim Step 19 | Train Loss: 0.4720 ]\n",
      "2025-07-08 05:45:49-3080550328.py:20-INFO >> [ Epoch 1 | idx: 39 | Optim Step 20 | Train Loss: 0.4560 ]\n",
      "2025-07-08 05:45:57-3080550328.py:20-INFO >> [ Epoch 1 | idx: 41 | Optim Step 21 | Train Loss: 0.8310 ]\n",
      "2025-07-08 05:46:05-3080550328.py:20-INFO >> [ Epoch 1 | idx: 43 | Optim Step 22 | Train Loss: 1.1667 ]\n",
      "2025-07-08 05:46:13-3080550328.py:20-INFO >> [ Epoch 1 | idx: 45 | Optim Step 23 | Train Loss: 0.4384 ]\n",
      "2025-07-08 05:46:20-3080550328.py:20-INFO >> [ Epoch 1 | idx: 47 | Optim Step 24 | Train Loss: 0.9284 ]\n",
      "2025-07-08 05:46:27-3080550328.py:20-INFO >> [ Epoch 1 | idx: 49 | Optim Step 25 | Train Loss: 0.9944 ]\n",
      "2025-07-08 05:46:35-3080550328.py:20-INFO >> [ Epoch 1 | idx: 51 | Optim Step 26 | Train Loss: 1.3407 ]\n",
      "2025-07-08 05:46:42-3080550328.py:20-INFO >> [ Epoch 1 | idx: 53 | Optim Step 27 | Train Loss: 0.2837 ]\n",
      "2025-07-08 05:46:50-3080550328.py:20-INFO >> [ Epoch 1 | idx: 55 | Optim Step 28 | Train Loss: 1.8523 ]\n",
      "2025-07-08 05:46:57-3080550328.py:20-INFO >> [ Epoch 1 | idx: 57 | Optim Step 29 | Train Loss: 0.8662 ]\n",
      "2025-07-08 05:47:05-3080550328.py:20-INFO >> [ Epoch 1 | idx: 59 | Optim Step 30 | Train Loss: 0.5618 ]\n",
      "2025-07-08 05:47:13-3080550328.py:20-INFO >> [ Epoch 1 | idx: 61 | Optim Step 31 | Train Loss: 0.5858 ]\n",
      "2025-07-08 05:47:20-3080550328.py:20-INFO >> [ Epoch 1 | idx: 63 | Optim Step 32 | Train Loss: 0.1506 ]\n",
      "2025-07-08 05:47:28-3080550328.py:20-INFO >> [ Epoch 1 | idx: 65 | Optim Step 33 | Train Loss: 0.9641 ]\n",
      "2025-07-08 05:47:36-3080550328.py:20-INFO >> [ Epoch 1 | idx: 67 | Optim Step 34 | Train Loss: 0.3899 ]\n",
      "2025-07-08 05:47:43-3080550328.py:20-INFO >> [ Epoch 1 | idx: 69 | Optim Step 35 | Train Loss: 1.1298 ]\n",
      "2025-07-08 05:47:51-3080550328.py:20-INFO >> [ Epoch 1 | idx: 71 | Optim Step 36 | Train Loss: 0.3887 ]\n",
      "2025-07-08 05:47:59-3080550328.py:20-INFO >> [ Epoch 1 | idx: 73 | Optim Step 37 | Train Loss: 0.1940 ]\n",
      "2025-07-08 05:48:06-3080550328.py:20-INFO >> [ Epoch 1 | idx: 75 | Optim Step 38 | Train Loss: 0.6001 ]\n",
      "2025-07-08 05:48:13-3080550328.py:20-INFO >> [ Epoch 1 | idx: 77 | Optim Step 39 | Train Loss: 1.5643 ]\n",
      "2025-07-08 05:48:21-3080550328.py:20-INFO >> [ Epoch 1 | idx: 79 | Optim Step 40 | Train Loss: 1.0147 ]\n",
      "2025-07-08 05:48:28-3080550328.py:20-INFO >> [ Epoch 1 | idx: 81 | Optim Step 41 | Train Loss: 0.8702 ]\n",
      "2025-07-08 05:48:35-3080550328.py:20-INFO >> [ Epoch 1 | idx: 83 | Optim Step 42 | Train Loss: 0.2519 ]\n",
      "2025-07-08 05:48:42-3080550328.py:20-INFO >> [ Epoch 1 | idx: 85 | Optim Step 43 | Train Loss: 1.0426 ]\n",
      "2025-07-08 05:48:50-3080550328.py:20-INFO >> [ Epoch 1 | idx: 87 | Optim Step 44 | Train Loss: 0.3419 ]\n",
      "2025-07-08 05:48:57-3080550328.py:20-INFO >> [ Epoch 1 | idx: 89 | Optim Step 45 | Train Loss: 0.3912 ]\n",
      "2025-07-08 05:49:05-3080550328.py:20-INFO >> [ Epoch 1 | idx: 91 | Optim Step 46 | Train Loss: 0.3445 ]\n",
      "2025-07-08 05:49:11-3080550328.py:20-INFO >> [ Epoch 1 | idx: 93 | Optim Step 47 | Train Loss: 0.8359 ]\n",
      "2025-07-08 05:49:19-3080550328.py:20-INFO >> [ Epoch 1 | idx: 95 | Optim Step 48 | Train Loss: 0.1028 ]\n",
      "2025-07-08 05:49:26-3080550328.py:20-INFO >> [ Epoch 1 | idx: 97 | Optim Step 49 | Train Loss: 0.3893 ]\n",
      "2025-07-08 05:49:34-3080550328.py:20-INFO >> [ Epoch 1 | idx: 99 | Optim Step 50 | Train Loss: 0.5409 ]\n",
      "2025-07-08 05:49:41-3080550328.py:20-INFO >> [ Epoch 1 | idx: 101 | Optim Step 51 | Train Loss: 0.5003 ]\n",
      "2025-07-08 05:49:48-3080550328.py:20-INFO >> [ Epoch 1 | idx: 103 | Optim Step 52 | Train Loss: 0.5085 ]\n",
      "2025-07-08 05:49:55-3080550328.py:20-INFO >> [ Epoch 1 | idx: 105 | Optim Step 53 | Train Loss: 0.3972 ]\n",
      "2025-07-08 05:50:03-3080550328.py:20-INFO >> [ Epoch 1 | idx: 107 | Optim Step 54 | Train Loss: 0.5083 ]\n",
      "2025-07-08 05:50:10-3080550328.py:20-INFO >> [ Epoch 1 | idx: 109 | Optim Step 55 | Train Loss: 1.1820 ]\n",
      "2025-07-08 05:50:18-3080550328.py:20-INFO >> [ Epoch 1 | idx: 111 | Optim Step 56 | Train Loss: 0.3679 ]\n",
      "2025-07-08 05:50:26-3080550328.py:20-INFO >> [ Epoch 1 | idx: 113 | Optim Step 57 | Train Loss: 1.0462 ]\n",
      "2025-07-08 05:50:33-3080550328.py:20-INFO >> [ Epoch 1 | idx: 115 | Optim Step 58 | Train Loss: 1.3001 ]\n",
      "2025-07-08 05:50:41-3080550328.py:20-INFO >> [ Epoch 1 | idx: 117 | Optim Step 59 | Train Loss: 0.3429 ]\n",
      "2025-07-08 05:50:49-3080550328.py:20-INFO >> [ Epoch 1 | idx: 119 | Optim Step 60 | Train Loss: 0.5169 ]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11acc10871d54ba88c473262678c463e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 05:54:28-3080550328.py:24-INFO >> Val Loss @ Optim step: 60 -> 1.4042\n",
      "\n",
      "2025-07-08 05:54:30-3080550328.py:29-INFO >> ***** ✅ Checkpoint Saved *****\n",
      "\n",
      "/home/aritrad/miniconda3/envs/stable_env/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/aritrad/miniconda3/envs/stable_env/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "2025-07-08 05:54:37-3080550328.py:20-INFO >> [ Epoch 1 | idx: 121 | Optim Step 61 | Train Loss: 0.5263 ]\n",
      "2025-07-08 05:54:44-3080550328.py:20-INFO >> [ Epoch 1 | idx: 123 | Optim Step 62 | Train Loss: 0.9249 ]\n",
      "2025-07-08 05:54:51-3080550328.py:20-INFO >> [ Epoch 1 | idx: 125 | Optim Step 63 | Train Loss: 0.7829 ]\n",
      "2025-07-08 05:54:59-3080550328.py:20-INFO >> [ Epoch 1 | idx: 127 | Optim Step 64 | Train Loss: 0.8294 ]\n",
      "2025-07-08 05:55:07-3080550328.py:20-INFO >> [ Epoch 1 | idx: 129 | Optim Step 65 | Train Loss: 0.6452 ]\n",
      "2025-07-08 05:55:15-3080550328.py:20-INFO >> [ Epoch 1 | idx: 131 | Optim Step 66 | Train Loss: 0.2739 ]\n",
      "2025-07-08 05:55:23-3080550328.py:20-INFO >> [ Epoch 1 | idx: 133 | Optim Step 67 | Train Loss: 0.4263 ]\n",
      "2025-07-08 05:55:30-3080550328.py:20-INFO >> [ Epoch 1 | idx: 135 | Optim Step 68 | Train Loss: 1.5521 ]\n",
      "2025-07-08 05:55:38-3080550328.py:20-INFO >> [ Epoch 1 | idx: 137 | Optim Step 69 | Train Loss: 0.5378 ]\n",
      "2025-07-08 05:55:46-3080550328.py:20-INFO >> [ Epoch 1 | idx: 139 | Optim Step 70 | Train Loss: 0.2747 ]\n",
      "2025-07-08 05:55:53-3080550328.py:20-INFO >> [ Epoch 1 | idx: 141 | Optim Step 71 | Train Loss: 0.8163 ]\n",
      "2025-07-08 05:55:59-3080550328.py:20-INFO >> [ Epoch 1 | idx: 143 | Optim Step 72 | Train Loss: 0.8049 ]\n",
      "2025-07-08 05:56:07-3080550328.py:20-INFO >> [ Epoch 1 | idx: 145 | Optim Step 73 | Train Loss: 0.5781 ]\n",
      "2025-07-08 05:56:13-3080550328.py:20-INFO >> [ Epoch 1 | idx: 147 | Optim Step 74 | Train Loss: 0.4533 ]\n",
      "2025-07-08 05:56:21-3080550328.py:20-INFO >> [ Epoch 1 | idx: 149 | Optim Step 75 | Train Loss: 0.2775 ]\n",
      "2025-07-08 05:56:28-3080550328.py:20-INFO >> [ Epoch 1 | idx: 151 | Optim Step 76 | Train Loss: 1.3275 ]\n",
      "2025-07-08 05:56:36-3080550328.py:20-INFO >> [ Epoch 1 | idx: 153 | Optim Step 77 | Train Loss: 0.6846 ]\n",
      "2025-07-08 05:56:44-3080550328.py:20-INFO >> [ Epoch 1 | idx: 155 | Optim Step 78 | Train Loss: 0.4606 ]\n",
      "2025-07-08 05:56:51-3080550328.py:20-INFO >> [ Epoch 1 | idx: 157 | Optim Step 79 | Train Loss: 0.5584 ]\n",
      "2025-07-08 05:56:59-3080550328.py:20-INFO >> [ Epoch 1 | idx: 159 | Optim Step 80 | Train Loss: 0.3814 ]\n",
      "2025-07-08 05:57:06-3080550328.py:20-INFO >> [ Epoch 1 | idx: 161 | Optim Step 81 | Train Loss: 0.3039 ]\n",
      "2025-07-08 05:57:13-3080550328.py:20-INFO >> [ Epoch 1 | idx: 163 | Optim Step 82 | Train Loss: 0.1828 ]\n",
      "2025-07-08 05:57:20-3080550328.py:20-INFO >> [ Epoch 1 | idx: 165 | Optim Step 83 | Train Loss: 0.4792 ]\n",
      "2025-07-08 05:57:28-3080550328.py:20-INFO >> [ Epoch 1 | idx: 167 | Optim Step 84 | Train Loss: 0.8582 ]\n",
      "2025-07-08 05:57:36-3080550328.py:20-INFO >> [ Epoch 1 | idx: 169 | Optim Step 85 | Train Loss: 0.9188 ]\n",
      "2025-07-08 05:57:44-3080550328.py:20-INFO >> [ Epoch 1 | idx: 171 | Optim Step 86 | Train Loss: 0.8100 ]\n",
      "2025-07-08 05:57:51-3080550328.py:20-INFO >> [ Epoch 1 | idx: 173 | Optim Step 87 | Train Loss: 0.8805 ]\n",
      "2025-07-08 05:57:59-3080550328.py:20-INFO >> [ Epoch 1 | idx: 175 | Optim Step 88 | Train Loss: 0.1929 ]\n",
      "2025-07-08 05:58:07-3080550328.py:20-INFO >> [ Epoch 1 | idx: 177 | Optim Step 89 | Train Loss: 1.1371 ]\n",
      "2025-07-08 05:58:14-3080550328.py:20-INFO >> [ Epoch 1 | idx: 179 | Optim Step 90 | Train Loss: 0.3572 ]\n",
      "2025-07-08 05:58:21-3080550328.py:20-INFO >> [ Epoch 1 | idx: 181 | Optim Step 91 | Train Loss: 0.3877 ]\n",
      "2025-07-08 05:58:29-3080550328.py:20-INFO >> [ Epoch 1 | idx: 183 | Optim Step 92 | Train Loss: 0.3972 ]\n",
      "2025-07-08 05:58:37-3080550328.py:20-INFO >> [ Epoch 1 | idx: 185 | Optim Step 93 | Train Loss: 1.3469 ]\n",
      "2025-07-08 05:58:45-3080550328.py:20-INFO >> [ Epoch 1 | idx: 187 | Optim Step 94 | Train Loss: 0.8219 ]\n",
      "2025-07-08 05:58:51-3080550328.py:20-INFO >> [ Epoch 1 | idx: 189 | Optim Step 95 | Train Loss: 0.5181 ]\n",
      "2025-07-08 05:58:59-3080550328.py:20-INFO >> [ Epoch 1 | idx: 191 | Optim Step 96 | Train Loss: 0.5632 ]\n",
      "2025-07-08 05:59:07-3080550328.py:20-INFO >> [ Epoch 1 | idx: 193 | Optim Step 97 | Train Loss: 0.3345 ]\n",
      "2025-07-08 05:59:14-3080550328.py:20-INFO >> [ Epoch 1 | idx: 195 | Optim Step 98 | Train Loss: 0.1580 ]\n",
      "2025-07-08 05:59:21-3080550328.py:20-INFO >> [ Epoch 1 | idx: 197 | Optim Step 99 | Train Loss: 0.5779 ]\n",
      "2025-07-08 05:59:29-3080550328.py:20-INFO >> [ Epoch 1 | idx: 199 | Optim Step 100 | Train Loss: 0.2757 ]\n",
      "2025-07-08 05:59:36-3080550328.py:20-INFO >> [ Epoch 1 | idx: 201 | Optim Step 101 | Train Loss: 0.4191 ]\n",
      "2025-07-08 05:59:44-3080550328.py:20-INFO >> [ Epoch 1 | idx: 203 | Optim Step 102 | Train Loss: 0.3494 ]\n",
      "2025-07-08 05:59:52-3080550328.py:20-INFO >> [ Epoch 1 | idx: 205 | Optim Step 103 | Train Loss: 1.0489 ]\n",
      "2025-07-08 05:59:59-3080550328.py:20-INFO >> [ Epoch 1 | idx: 207 | Optim Step 104 | Train Loss: 1.2299 ]\n",
      "2025-07-08 06:00:07-3080550328.py:20-INFO >> [ Epoch 1 | idx: 209 | Optim Step 105 | Train Loss: 0.5269 ]\n",
      "2025-07-08 06:00:14-3080550328.py:20-INFO >> [ Epoch 1 | idx: 211 | Optim Step 106 | Train Loss: 0.3176 ]\n",
      "2025-07-08 06:00:22-3080550328.py:20-INFO >> [ Epoch 1 | idx: 213 | Optim Step 107 | Train Loss: 1.6098 ]\n",
      "2025-07-08 06:00:30-3080550328.py:20-INFO >> [ Epoch 1 | idx: 215 | Optim Step 108 | Train Loss: 0.9579 ]\n",
      "2025-07-08 06:00:38-3080550328.py:20-INFO >> [ Epoch 1 | idx: 217 | Optim Step 109 | Train Loss: 0.3813 ]\n",
      "2025-07-08 06:00:45-3080550328.py:20-INFO >> [ Epoch 1 | idx: 219 | Optim Step 110 | Train Loss: 0.6098 ]\n",
      "2025-07-08 06:00:53-3080550328.py:20-INFO >> [ Epoch 1 | idx: 221 | Optim Step 111 | Train Loss: 0.1785 ]\n",
      "2025-07-08 06:01:00-3080550328.py:20-INFO >> [ Epoch 1 | idx: 223 | Optim Step 112 | Train Loss: 0.6191 ]\n",
      "2025-07-08 06:01:07-3080550328.py:20-INFO >> [ Epoch 1 | idx: 225 | Optim Step 113 | Train Loss: 0.6216 ]\n",
      "2025-07-08 06:01:14-3080550328.py:20-INFO >> [ Epoch 1 | idx: 227 | Optim Step 114 | Train Loss: 0.4892 ]\n",
      "2025-07-08 06:01:22-3080550328.py:20-INFO >> [ Epoch 1 | idx: 229 | Optim Step 115 | Train Loss: 0.6309 ]\n",
      "2025-07-08 06:01:30-3080550328.py:20-INFO >> [ Epoch 1 | idx: 231 | Optim Step 116 | Train Loss: 1.1524 ]\n",
      "2025-07-08 06:01:38-3080550328.py:20-INFO >> [ Epoch 1 | idx: 233 | Optim Step 117 | Train Loss: 0.4738 ]\n",
      "2025-07-08 06:01:46-3080550328.py:20-INFO >> [ Epoch 1 | idx: 235 | Optim Step 118 | Train Loss: 0.7494 ]\n",
      "2025-07-08 06:01:54-3080550328.py:20-INFO >> [ Epoch 1 | idx: 237 | Optim Step 119 | Train Loss: 1.4992 ]\n",
      "2025-07-08 06:02:02-3080550328.py:20-INFO >> [ Epoch 1 | idx: 239 | Optim Step 120 | Train Loss: 0.2600 ]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb7e94cb1f4412783da2a6de40478e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        inputs, labels = batch\n",
    "        outputs = qdora_qwen_model(**inputs, labels=labels)\n",
    "        loss = outputs.loss / gradient_accumulation_steps\n",
    "    \n",
    "        loss.backward()\n",
    "        accumulated_loss += loss.item()\n",
    "        \n",
    "        if (idx+1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            logger.info(f\"[ Epoch {epoch+1} | idx: {idx} | Optim Step {global_step} | Train Loss: {loss.item():.4f} ]\")\n",
    "\n",
    "            if global_step % 60 == 0:\n",
    "                avg_val_loss = do_validation()\n",
    "                logger.info(f\"Val Loss @ Optim step: {global_step} -> {avg_val_loss:.4f}\\n\")\n",
    "            \n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    qdora_qwen_model.save_pretrained(os.path.join(saveDir, 'Qwen-MultiDomain-QDORA-chkpt-16R.pt'))\n",
    "                    logger.info(f\"***** ✅ Checkpoint Saved *****\\n\")\n",
    "\n",
    "    logger.info(f\"Epoch {epoch+1} completed. Avg loss: {accumulated_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56751471-64d4-4750-81c0-bdf86d0e32c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa32ff-763b-497d-ba2d-57ab5a9fb37f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
