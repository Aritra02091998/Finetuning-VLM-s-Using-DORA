{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a4050f-2a0e-4685-8f6f-8f707293bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import datasets\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432e7fa7-b9b2-4f9a-9517-fad347562c5f",
   "metadata": {},
   "source": [
    "### Set Up Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfb53885-7b06-4758-a45a-e4d60f62f700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 06:27:09,558 - INFO - Logging is set up in the notebook!\n"
     ]
    }
   ],
   "source": [
    "# Clear previous handlers to avoid duplicate logs in Jupyter\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Change to DEBUG for more verbosity\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]  # Ensures it logs to Jupyter cell output\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Logging is set up in the notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc497182-14b1-4ac4-9331-d258b5e59a6e",
   "metadata": {},
   "source": [
    "### Load the MultiDomain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "984e34c5-cb23-471e-8a4a-c919bff96958",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Generate a one word or single number answer for the given image and question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3afe3548-57fd-4d1d-94bc-e489d95948e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_prefix(example):\n",
    "    example['question'] = prefix + ': ' + example['question']\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6b83a92-14dd-4acc-b71e-c7ecc3173f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dutta18/multi-domain-VQA-1.5K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd790a78-b3f0-409a-b50e-bf5e7363298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = dataset['train'], dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe391a9-19b2-4b9b-aab0-0b63d1f42acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.map(prepend_prefix)\n",
    "val_set = val_set.map(prepend_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e49164f9-0e27-4d1a-a591-0dab931641da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 600)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07600f1d-364a-42e5-a473-2672c1c1867c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c693161-a8e2-4da5-9986-2c870ab56b60",
   "metadata": {},
   "source": [
    "### Importing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fcd22f3-174d-4ddf-844c-9ef45519b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, Idefics3ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87117fe7-e7d0-4b50-917d-aa46914d1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceTB/SmolVLM-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c1804c2-1966-44c5-ba98-b5014df87b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b7a649-aac1-4aad-86e2-2370c5d78833",
   "metadata": {},
   "source": [
    "### Initialize Quantisation Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c38a3c9f-d5dd-491c-94a0-e1c1e32a19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4384a1-d6b8-487c-bc2d-1f0c3545e60e",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d20ed1d5-8342-4ac6-becd-0a8706b13266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: image_seq_len. \n"
     ]
    }
   ],
   "source": [
    "model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.bfloat16, \n",
    "    _attn_implementation = \"flash_attention_2\",\n",
    "    device_map = 'auto'\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "977baa5d-ec51-4db9-9f27-b844b049e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c96725-c1b1-41e0-b9d1-fd71effa260b",
   "metadata": {},
   "source": [
    "### Intialize DORA Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdbc0777-8a16-4055-997f-6fc693cee6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dora_config = LoraConfig(\n",
    "    r = 16,\n",
    "    lora_alpha = 16*2,\n",
    "    lora_dropout = 0.05,\n",
    "    target_modules = ['o_proj','k_proj','q_proj', 'v_proj'],\n",
    "    init_lora_weights = \"gaussian\",\n",
    "    inference_mode = False,\n",
    "    use_dora = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2329fc0-153b-44c2-b409-40b587b8f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "smolvlm_qdora_model = get_peft_model(model, dora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d44520-4b78-4f3d-85e2-06c9db421eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da638b9c-0544-4dc1-a741-759384d3f247",
   "metadata": {},
   "source": [
    "### Report the Trainable Params: ~ 9.6 M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "443fd718-63a0-4516-b855-9b7052a6d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_trainable_params():\n",
    "    \n",
    "    # Simple param report\n",
    "    trainable = sum(p.numel() for p in smolvlm_qdora_model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable params: {trainable/1e6:.1f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c49332b8-c061-473c-acd9-095d5a19eb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable params: 9.6 M\n"
     ]
    }
   ],
   "source": [
    "report_trainable_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8f44822-4a49-4081-8eb8-61bb1f07d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_token_id = processor.tokenizer.additional_special_tokens_ids[processor.tokenizer.additional_special_tokens.index(\"<image>\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5a035b3-30b0-4d84-a4c5-80956bc9b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "\n",
    "    for example in examples:\n",
    "        image = example[\"image\"]\n",
    "        question = example[\"question\"]\n",
    "        answer = example[\"answer\"]\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Answer briefly.\"},\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": question}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": answer}]\n",
    "            }\n",
    "        ]\n",
    "        text = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "        texts.append(text.strip())\n",
    "        images.append([image])\n",
    "\n",
    "    # Batch using processor\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Manually set labels\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    # Now cast pixel_values explicitly\n",
    "    batch[\"pixel_values\"] = batch[\"pixel_values\"].to(torch.bfloat16)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451915f-df5f-4e03-a083-3ad6ba071f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8c9def8-ade8-485f-8dfa-61bfdfc59c05",
   "metadata": {},
   "source": [
    "### Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d709cb44-5415-4d53-950f-678765fc678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def do_validation():\n",
    "\n",
    "    smolvlm_qdora_model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs = smolvlm_qdora_model.eval()(**batch)\n",
    "            \n",
    "        loss = outputs[\"loss\"]\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    smolvlm_qdora_model.eval().train()\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f76b0e-6fa6-45d2-8914-1dbfb81c3207",
   "metadata": {},
   "source": [
    "### Training Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6def5872-1197-4ae5-8485-e5c07abe9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b56c5e7-30f0-4723-b090-406e8549d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ = 4\n",
    "max_epochs = 10\n",
    "grad_accum_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03b9a1af-7058-41ea-a064-16e5f77fa8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=batch_, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6dc7c5f-b846-4261-87e3-7cac71e6339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(smolvlm_qdora_model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "841aebfa-1a58-4195-b416-213aad21becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "best_val_loss = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe981701-b41f-4501-b8f5-36bcac7aeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = smolvlm_qdora_model.train()\n",
    "smolvlm_qdora_model.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca0b50-1c6e-4b7f-bff4-22b2611683d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e87b7554-e43c-4b0c-9713-9bc3c3f1d567",
   "metadata": {},
   "source": [
    "## Native PyTorch Training Loop\n",
    "\n",
    "##### I am using val_loss as the checkpointing criteria, but any other metric which test text generation quality can be used here.\n",
    "\n",
    "##### MAX GPU USAGE = 23 GB on NVIDIA A40 Card (Adjust LORA Rank, batch size, grad_accum_steps accordingly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c26ec83-a7e2-4dbd-a6cb-1c2f85fbaa79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c2754bd7b94429a177863e239cf684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/aritrad/miniconda3/envs/stable_env/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "2025-07-08 06:28:05,054 - INFO - [ Epoch 1 | idx: 1 | Optim Step 1 | Train Loss: 0.6868 ]\n",
      "2025-07-08 06:28:20,267 - INFO - [ Epoch 1 | idx: 3 | Optim Step 2 | Train Loss: 0.6764 ]\n",
      "2025-07-08 06:28:35,542 - INFO - [ Epoch 1 | idx: 5 | Optim Step 3 | Train Loss: 0.7361 ]\n",
      "2025-07-08 06:28:51,724 - INFO - [ Epoch 1 | idx: 7 | Optim Step 4 | Train Loss: 0.6870 ]\n",
      "2025-07-08 06:29:06,961 - INFO - [ Epoch 1 | idx: 9 | Optim Step 5 | Train Loss: 0.7205 ]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(max_epochs)):  \n",
    "    \n",
    "    accumulated_loss = 0\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "        with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs = smolvlm_qdora_model(**batch)\n",
    "            loss = outputs[\"loss\"] / grad_accum_steps\n",
    "\n",
    "        loss.backward()\n",
    "        accumulated_loss += loss.item()\n",
    "        \n",
    "        if (idx + 1) % grad_accum_steps == 0: \n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            logger.info(f\"[ Epoch {epoch+1} | idx: {idx} | Optim Step {global_step} | Train Loss: {loss.item():.4f} ]\")\n",
    "\n",
    "            if global_step % 60 == 0:\n",
    "                avg_val_loss = do_validation()\n",
    "                logger.info(f\"Val Loss @ Optim step: {global_step} -> {avg_val_loss:.4f}\\n\")\n",
    "            \n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    smolvlm_qdora_model.save_pretrained('../chkpts/SmolVLM-MultiDomain-QDORA-chkpt-16R.pt')\n",
    "                    logger.info(f\"***** ✅ Checkpoint Saved *****\\n\")\n",
    "    \n",
    "    scheduler.step() \n",
    "    logger.info(f\"Epoch {epoch+1} completed. Avg loss: {accumulated_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328abd94-0f6c-460a-82da-85bc91cd6fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f1b3b-483d-4f91-bd1e-641243edd05f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
